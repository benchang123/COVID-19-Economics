---
title: "Interim Write Up"
author: "Nicolas Min"
date: "3/21/2021"
output:
  pdf_document: default
  html_document:
    df_print: paged
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ggplot2)
library(dplyr)
library(stringr)
library(gridExtra)
library(reshape2)
library(readxl)
library(lubridate)
library(zoo)
library(sjPlot)
library(sjmisc)
library(sjlabelled)
library(knitr)
library(tidyr)
library(broom)
library(stargazer)

#Obtain working directory
wd <- dirname(rstudioapi::getSourceEditorContext()$path) #get rmd file path
wd <- ifelse(substr(wd,nchar(wd)-3,nchar(wd)) == 'Code', substr(wd,1,nchar(wd)-5), wd)
setwd(wd) #set working directory
```

## 1. Problem Description

As a result of the COVID-19 crisis and the various non-pharmaceutical interventions (NPIs) to contain the spread of SARS-CoV-2, the U.S. saw an unprecedented spike in its claims for unemployment. And because various NPIs have been implemented at different levels in a decentralized manner, the U.S. saw significant geographic and temporal variations in its mobility and change in unemployment.

It now has been a year since everything suddenly froze in February of 2020. On a positive note, as of February 2021, the U.S. unemployment rate decreased to 6.2 percent from the record-high 14.8 percent in April 2020. In particular, according to "The Employment Situation - February 2021" released by the Bureau of Labor Statistics, employment in leisure and hospitality increased by 355,000 as "pandemic-related restrictions eased in some parts of the country". As things start to recover, people ask: "when will things get back to normal?" In the context of economics, the question becomes: "when will the U.S. recover the stable, low unemployment rate of 4% that we saw back in February 2020?" Our project intends to answer this question.

Unemployment is a major determinant of a strong aggregate demand and a healthy labor market. In this project, we will contrive a model that predicts U.S. unemployment in April/May 2021, a phase which we anticipate to see a significant decrease in COVID cases, thanks to the seasonality of the virus (assuming Holmstrom 2021 is correct) and thanks to the rapid vaccinations taking place throughout the nation. More precisely, our model will predict the *change in unemployment rate* in April/May 2021 for a given industrial sector for a given state. To do this, we will include the following variables in our model: mobility (where people are located), stringency (how strict NPIs are in a given state), sector composition (e.g., construction, financial services, leisure), and other control variables (such as the new COVID cases). Since we will be using seasonally adjusted data, we assume that our predictive model, composed of the specified variables, will generally hold true until the end of the pandemic.

## 2. Data Description
Below we specify the data we used for the variables in our model.

### 2-1. Mobility - Google Mobility data
Since the dawn of the pandemic, Google has been collecting global mobility data to inform public health officials' decisions to combat COVID-19. For each day at a given state, Google measures relative change (not the absolute visitors nor the duration) in mobility to the pre-COVID baseline level which is the median visitors to a given location from the 5-week period from Jan 3rd - Feb 6th, 2020. The entire data is relies on the Google users who have their Location History switched on. If these users visit categorized places (e.g., Parks - public garden, castle, camp ground; Transit stations - subway, taxi stand, car rental agency), Google aggregates the number of these users' location information to compute a mobility measure.

In our study, we will be focusing on the mobility data of the fifty U.S. states. As of 3/16/2021, the mobility dataset for the U.S. has 1,007,523 rows and contains retail, grocery, parks, transit, workplace, residential mobility data for each state at any given date between 2/15/2020 and 3/16/2021. (for further details, see: https://support.google.com/covid19-mobility/answer/9825414?hl=en&ref_topic=9822927).

One possible limitation of this data, as pointed out in a relevant study ("Unemployment Effects of Stay-at-Home Orders:Evidence from High Frequency Claims Data'' - Baek et al. (2020)) is that the data is derived only from those with Google Accounts who opt into Location History services. However, we agree with Baek that the selection bias is unlikely to be a major concern "given Google's broad reach... (there are over 1.5 billion Gmail accounts)". 

Assuming that the Google Mobility data is unbiased and accurate, we believe that the mobility information gathered from the data will serve as a good predictor of unemployment because of the direct and indirect relationship that mobility has with unemployment. Directly, less mobility would mean less demand (less transactions for over-the-counter goods, for instance), so workplaces would need less laborers. Indirectly, the shutdowns and the fear of the pandemic would lead to a drop in mobility and to an increase in unemployment. 

Based on **Figure 1.**, we can observe that indeed mobility and unemployment have connections for many states. Namely, in most of the states, when the unemployment rate is high, the retail/recreational mobility is generally low (while residential mobility is high), but when the unemployment rate is low, the retail/recreational mobility is generally high (while residential mobility is low).

Finally, we noticed that the level of unemployment rose the most for Nevada, a state well known for its hospitality and leisure business, during the initial phase of the pandemic. This fact made us question whether a high dependency on person-to-person business makes a state's employment more vulnerable to the pandemic. We will delve deeper into this as we progress.

(Note: the unemployment data used here for the purpose of EDA is not the actual unemployment data we will use in our predictive model.)

### Figure 1.
```{r echo=FALSE, out.width=c('50%', '50%'), fig.show='hold'}
#change wd
#setwd("../COVID-19-Economics")

### Unemployment per state data ###
state_data <- read.delim('https://download.bls.gov/pub/time.series/la/la.data.3.AllStatesS')

#Create index of series 
index <- read.delim('https://download.bls.gov/pub/time.series/la/la.series') %>% 
  filter(seasonal=='S'& grepl('LASST', series_id) & measure_code == 3) %>%
  select(c(series_id,series_title,srd_code))
states_index <- read.delim('https://download.bls.gov/pub/time.series/la/la.state_region_division')

index <- index %>% merge(states_index)

#Specify states
states <- c('California','New York', 'Nevada', 'Florida', 'Hawaii')
series <- index %>% filter(srd_text %in% states) %>% select(series_id, srd_text)

#Obtain specific data series
state_data_sp <- state_data %>% filter(series_id %in% unlist(series), year >= 2019) %>% 
  merge(series) %>%
  select(c(srd_text,year, period, value))

#Date format
dates <- as.Date(paste0(state_data_sp$year,substr(state_data_sp$period, 2,3),'01'),format='%Y%m%d')

state_data_sp <- state_data_sp %>% mutate(date = dates) %>%
  select(c(date, srd_text, value)) %>%
  'colnames<-'(c('Date','State','UR'))

### Mobility data ###

mobility <- read.csv('Data/2020_US_Region_Mobility_Report.csv', header = T, na.strings ="") %>% 
  mutate(date = as.Date(date)) %>% filter(sub_region_1 %in% states & is.na(sub_region_2)) %>% 
  select(c(date, sub_region_1, retail_and_recreation_percent_change_from_baseline,
                                  grocery_and_pharmacy_percent_change_from_baseline, parks_percent_change_from_baseline,
                                  transit_stations_percent_change_from_baseline, workplaces_percent_change_from_baseline,
                                  residential_percent_change_from_baseline)) %>% 
  'colnames<-'(c('Date','State','Mob_ret_rec','Mob_gro_pha', 'Mob_parks','Mob_transit','Mob_work','Mob_res'))

#Merge both data bases
merge_data <- left_join(mobility, state_data_sp, by = c('Date','State'), all.x = TRUE)

#Fill Unemployment Rates gaps
last_UR <- state_data_sp %>% filter(Date=='2020-02-01') %>% select(UR) %>% unlist() %>% as.numeric() #obtain last ur data
merge_data[merge_data$Date=='2020-02-15','UR'] <- last_UR #replace last UR data available for February 15, 2020
merge_data <- merge_data %>% mutate(UR = as.numeric(na.locf(UR))) #Replace NAs with previous values


## Plots

colors <- c("Unemployment Rate" = "darkblue", "Mobility. Residential" = "red", 'Mobility. Retail and Recreation' = "purple",
            'Mobility. Grocery and Pharmacies'='darkgreen')

#California
merge_data %>% filter(State=='California') %>% 
  ggplot(aes(x=Date)) + 
  geom_line(aes(y=UR, color='Unemployment Rate'), size=1)+
  geom_line(aes(y=Mob_res, color='Mobility. Residential'),linetype='dashed', size=0.5)+
  geom_line(aes(y=Mob_ret_rec, color='Mobility. Retail and Recreation'),linetype='dashed', size=0.5)+
  geom_line(aes(y=Mob_gro_pha, color='Mobility. Grocery and Pharmacies'),linetype='dashed', size=0.5)+
  labs(x = 'Date', y = '% (Unemployment rate) / Index (Mobility)',
       title = 'California. Unemployment rate (solid line) and Mobility index (dashed lines)',
       color='Legend')+
  theme_bw()+
  scale_color_manual(values=colors)

#New York
merge_data %>% filter(State=='New York') %>% 
  ggplot(aes(x=Date)) + 
  geom_line(aes(y=UR, color='Unemployment Rate'), size=1)+
  geom_line(aes(y=Mob_res, color='Mobility. Residential'),linetype='dashed', size=0.5)+
  geom_line(aes(y=Mob_ret_rec, color='Mobility. Retail and Recreation'),linetype='dashed', size=0.5)+
  geom_line(aes(y=Mob_gro_pha, color='Mobility. Grocery and Pharmacies'),linetype='dashed', size=0.5)+
  labs(x = 'Date', y = '% (Unemployment rate) / Index (Mobility)',
       title = 'New York. Unemployment rate (solid line) and Mobility index (dashed lines)',
       color='Legend')+
  theme_bw()+
  scale_color_manual(values=colors)
```


```{r echo=FALSE, out.width=c('50%', '50%'), fig.show='hold'}
#Florida
merge_data %>% filter(State=='Florida') %>% 
  ggplot(aes(x=Date)) + 
  geom_line(aes(y=UR, color='Unemployment Rate'), size=1)+
  geom_line(aes(y=Mob_res, color='Mobility. Residential'),linetype='dashed', size=0.5)+
  geom_line(aes(y=Mob_ret_rec, color='Mobility. Retail and Recreation'),linetype='dashed', size=0.5)+
  geom_line(aes(y=Mob_gro_pha, color='Mobility. Grocery and Pharmacies'),linetype='dashed', size=0.5)+
  labs(x = 'Date', y = '% (Unemployment rate) / Index (Mobility)',
       title = 'Florida. Unemployment rate (solid line) and Mobility index (dashed lines)',
       color='Legend')+
  theme_bw()+
  scale_color_manual(values=colors)


#Hawaii
merge_data %>% filter(State=='Hawaii') %>% 
  ggplot(aes(x=Date)) + 
  geom_line(aes(y=UR, color='Unemployment Rate'), size=1)+
  geom_line(aes(y=Mob_res, color='Mobility. Residential'),linetype='dashed', size=0.5)+
  geom_line(aes(y=Mob_ret_rec, color='Mobility. Retail and Recreation'),linetype='dashed', size=0.5)+
  geom_line(aes(y=Mob_gro_pha, color='Mobility. Grocery and Pharmacies'),linetype='dashed', size=0.5)+
  labs(x = 'Date', y = '% (Unemployment rate) / Index (Mobility)',
       title = 'Hawaii. Unemployment rate (solid line) and Mobility index (dashed lines)',
       color='Legend')+
  theme_bw()+
  scale_color_manual(values=colors)
```


```{r echo=FALSE, out.width=c('50%'), fig.show='hold'}
#Nevada
merge_data %>% filter(State=='Nevada') %>% 
  ggplot(aes(x=Date)) + 
  geom_line(aes(y=UR, color='Unemployment Rate'), size=1)+
  geom_line(aes(y=Mob_res, color='Mobility. Residential'),linetype='dashed', size=0.5)+
  geom_line(aes(y=Mob_ret_rec, color='Mobility. Retail and Recreation'),linetype='dashed', size=0.5)+
  geom_line(aes(y=Mob_gro_pha, color='Mobility. Grocery and Pharmacies'),linetype='dashed', size=0.5)+
  labs(x = 'Date', y = '% (Unemployment rate) / Index (Mobility)',
       title = 'Nevada. Unemployment rate (solid line) and Mobility index (dashed lines)',
       color='Legend')+
  theme_bw()+
  scale_color_manual(values=colors)

```

### 2-2. Stringency - Oxford Stringency Data

The Oxford COVID-19 Government Response Tracker (OxCGRT) systematically collects global information on several different common policy responses that governments have taken to respond to the pandemic. OxCGRT focuses on 18 indicators, including, but not limited to, school closures, travel restrictions, and stay-at-home orders. For the fifty U.S. states, OxCGRT focuses on 16 of these indicators. Of these 16, OxCGRT utilizes 9 ordinal indicators (school closing, workplace closing, cancel public events, restrictions on gathering size, close public transport, stay at home requirements, restrictions on internal movement, restrictions on international travel, and public information campaign) to come up with the *Stringency Index* for each state at a given date. In terms of the dimension of the data set, there are 23192 observations in the dataset as of 3/21/2021.

The data, as reported by OxCGRT, is "collected from publicly available sources such as news articles and government press releases and briefings" which are then "identified via internet searches by a team of over 50 Oxford" affiliates. In fact, for each indicator, there is an attached note that reasons why OxCGRT assigned the index it assigned for the given state at a given date. For instance, for 08/20/2020 Wyoming, OxCGRT assigned a '3' (highest measure) for the school closing index, quoting from a news article, "Wyoming schools will start under a 3 tier system under which schools can choose themselves which tier they will be in". 

The one concern we have about this dataset is the underlying method of data collection. Investigating the attached notes, we found notes like: "While I can't find news sources to confirm... the Omaha Public Schools appear to be open in-person...". OxCGRT then assigned a '1' for Nebraska on 10/19/2020 for its school closing index. From these notes, we realized that there could be a chance that the qualitative judgment involved in the data collection might cause distortion in the stringency measure. However, given that over 50 people are involved in this judgment, we assumed there would be no systematic bias in the stringency measure. Also, we considered these judgments to be generally reasonable given that most of them (except for few) relied on the local government's official announcements.

Below, in **Figure 2.**, we have plotted two curves, unemployment and stringency, for each state. As expected, stringency and unemployment had significant relationships. Not only did we find that they follow a similar trend, we noticed that the peaks and troughs of stringency and unemployment match well for each state we observed. From this exercise, we could verify that Baek et al (2020)'s conclusion holds even for an extended time period beyond April 2020. Namely, we could verify that governmental orders to combat COVID have a significant impact on the unemployment rate throughout the era of COVID-19 pandemic.

Finally, the fact that the two curves matched particularly well for Nevada helped us confirm our thoughts from the previous section that there could be a high chance that mobility/stringency has a greater impact on the unemployment for those states with higher dependency on person-to-person oriented industries. Hence, in our next dataset, we look at the sectoral composition of each state.

(Note: the unemployment data used here for the purpose of EDA is not the actual unemployment data we will use in our predictive model.)

### Figure 2.
```{r echo=FALSE}
#load data
stringency_us <- read.csv("https://raw.githubusercontent.com/OxCGRT/USA-covid-policy/master/data/OxCGRT_US_latest.csv")
stringency_us <- stringency_us %>% select(Date, RegionName, StringencyIndex) %>% filter(RegionName != '')

#Modify name of one state
stringency_us[stringency_us$RegionName == 'Washington DC',]$RegionName <- rep('District of Columbia', length(stringency_us[stringency_us$RegionName == 'Washington DC',]$RegionName))

#Convert dates
stringency_us <- stringency_us %>% mutate(date = as.Date(as.character(Date), '%Y%m%d')) %>% 
  select(date, RegionName, StringencyIndex) %>% 
  'colnames<-'(c('date','state_name', 'stringency_index'))

#change col names
colnames(stringency_us) <- c("Date", "State", "Stringency_Index")

#Merge both data bases
merge_data2 <- left_join(stringency_us, state_data_sp, by = c('Date','State'), all.x = TRUE)
#merge_data2 <- merge_data2[!is.na(merge_data2$UR),]
```

```{r echo=FALSE, message = FALSE, warning = FALSE, out.width=c('50%', '50%'), fig.show='hold'}
#California
ggplot(data = merge_data2[merge_data2$State=="California",], aes(x=Date)) + 
  geom_line(data = merge_data2[merge_data2$State=="California" & !is.na(merge_data2$UR),], aes(y=UR, group=1, color='Unemployment Rate'), size=1)+
  geom_line(aes(y=Stringency_Index, color='Stringency Index'),linetype='dashed', size=0.5)+
  labs(x = 'Date', y = '% (Unemployment rate) / Index (Stringency)',
       title = 'California Unemployment rate (solid line) and Stringency index (dashed lines)',
       color='Legend')+
  theme_bw()

#New York
ggplot(data = merge_data2[merge_data2$State=="New York",], aes(x=Date)) + 
  geom_line(data = merge_data2[merge_data2$State=="New York" & !is.na(merge_data2$UR),], aes(y=UR, group=1, color='Unemployment Rate'), size=1)+
  geom_line(aes(y=Stringency_Index, color='Stringency Index'),linetype='dashed', size=0.5)+
  labs(x = 'Date', y = '% (Unemployment rate) / Index (Stringency)',
       title = 'New York Unemployment rate (solid line) and Stringency index (dashed lines)',
       color='Legend')+
  theme_bw()

```


```{r echo=FALSE, , message = FALSE, warning = FALSE, out.width=c('50%', '50%'), fig.show='hold'}
#Florida
ggplot(data = merge_data2[merge_data2$State=="Florida",], aes(x=Date)) + 
  geom_line(data = merge_data2[merge_data2$State=="Florida" & !is.na(merge_data2$UR),], aes(y=UR, group=1, color='Unemployment Rate'), size=1)+
  geom_line(aes(y=Stringency_Index, color='Stringency Index'),linetype='dashed', size=0.5)+
  labs(x = 'Date', y = '% (Unemployment rate) / Index (Stringency)',
       title = 'Florida Unemployment rate (solid line) and Stringency index (dashed lines)',
       color='Legend')+
  theme_bw()

#Hawaii
ggplot(data = merge_data2[merge_data2$State=="Hawaii",], aes(x=Date)) + 
  geom_line(data = merge_data2[merge_data2$State=="Hawaii" & !is.na(merge_data2$UR),], aes(y=UR, group=1, color='Unemployment Rate'), size=1)+
  geom_line(aes(y=Stringency_Index, color='Stringency Index'),linetype='dashed', size=0.5)+
  labs(x = 'Date', y = '% (Unemployment rate) / Index (Stringency)',
       title = 'Hawaii Unemployment rate (solid line) and Stringency index (dashed lines)',
       color='Legend')+
  theme_bw()
```


```{r echo=FALSE, message = FALSE, warning = FALSE, out.width=c('50%'), fig.show='hold'}
#Nevada
ggplot(data = merge_data2[merge_data2$State=="Nevada",], aes(x=Date)) + 
  geom_line(data = merge_data2[merge_data2$State=="Nevada" & !is.na(merge_data2$UR),], aes(y=UR, group=1, color='Unemployment Rate'), size=1)+
  geom_line(aes(y=Stringency_Index, color='Stringency Index'),linetype='dashed', size=0.5)+
  labs(x = 'Date', y = '% (Unemployment rate) / Index (Stringency)',
       title = 'Nevada Unemployment rate (solid line) and Stringency index (dashed lines)',
       color='Legend')+
  theme_bw()
```


### 2-3. Unemployment by state and sector - data from BLS data directory
From the Bureau of Labor Statistics data directory, we imported employment data for each state for each industrial sector (13 sectors, including, but not limited to: Mining, Construction, Information, Financial Activities, and Leisure) at a given month-end date (it goes without saying that such data serve as a foundation for many statistic and economic analysis in various fields). As of 3/31/2021, the data includes 8,169,164 rows (inception year is 1939!).

The "series ID" column contains information about the sector and the state. Using this information, we re-structured our dataset such that each row corresponds to each state and each sector. In addition, we manipulated our data so that we have the *monthly change in employment* per state and per sector.

One issue with the current dataset is that it currently reports not the unemployment rate, but the number of employees in absolute numbers. Going forward, we would need to convert the number of employees into the unemployment rate, by dividing our absolute numbers by the number of people in the labor force at a given period of time. With further investigation, we plan to obtain the labor force numbers from BLS data directory. For the time being, however, the analysis in this report is made using the *number of employees* in a given state, for a given sector, and at a given month-end date. So, the *change in employment* in this report is computed assuming that the people in the labor force in a given state and for a given sector are held constant for the two months being compared.

The below plots (**Figure 3.**) observe the time trend in employment by sector, over the course of the pandemic. The plots are indeed congruent with our hypothesis that the sectors that depend heavily on person-to-person contact were more vulnerable to the mobility restrictions due to the pandemic. For instance, Government or Financial Activities sectors were less impacted by the pandemic. On the other hand, Retail Trade or Leisure and Hospitality sectors were significantly impacted by the pandemic and the relevant governmental NPIs. Also, we see that different states saw different levels of changes in unemployment due to the pandemic. In particular, the states with a greater portion of its people working in sectors that require person-to-person contact (e.g., Nevada or New York) saw a greater change in unemployment over the course of the pandemic compared to the states that do not (e.g., Arkansas or Utah).
```{r include=FALSE}
######################################
######## Employment data #############
######################################
#change wd
setwd("../COVID-19-Economics")

# Source: Bureau of Labor Statistics
# Link: https://download.bls.gov/pub/time.series/sm/sm.data.1.AllData
# Series description: The original series are the SA thousand of employees at each month by sector and state

#Read data
all_data <- read.delim('https://download.bls.gov/pub/time.series/sm/sm.data.1.AllData')

#Create index of series 
index <- read.delim('https://download.bls.gov/pub/time.series/sm/sm.series') %>% 
  filter(seasonal=='S', area_code == 00000, data_type_code == 01, 
         industry_code == 1000000*supersector_code, state_code <= 56) %>%
  select(c(series_id,state_code,supersector_code)) %>%
  filter(supersector_code %in% c(10, 20, 30, 41, 42, 43, 50, 55, 60, 65, 70, 80, 90))
states_index <- read.delim('https://download.bls.gov/pub/time.series/sm/sm.state')
supersector_index <- read.delim('https://download.bls.gov/pub/time.series/sm/sm.supersector')


index <- index %>% merge(states_index, by='state_code') %>% merge(supersector_index, by='supersector_code') %>%
  select(series_id, state_name, supersector_name)


##Filter data
series <- unique(index[,1])
filtered_data <- all_data %>% filter(series_id %in% series) %>% merge(index, by='series_id') %>% 
  mutate(month = substr(period,2,3)) %>%
  filter(year >= 2019) %>%
  select(value, month, year, state_name, supersector_name)


##Transform data

#Obtain monthly change and drop off 2019 (jan-nov)
employment_data <- filtered_data %>% mutate(emp_monthly_change = as.numeric(value)/lag(as.numeric(value))-1) %>%
  mutate(year_month = paste0(year,month)) %>%
  filter(year_month >= 201912) %>%
  select(value, emp_monthly_change, month, year, state_name, supersector_name)

#Built date variable
dates <- as.Date(as.yearmon(paste(employment_data$year, employment_data$month, sep='-')), frac = 1)
employment_data <- employment_data %>% mutate(date = dates) %>% select(date, value, emp_monthly_change, state_name, supersector_name) 

colnames(employment_data)[2] <- 'employees'

head(employment_data)
tail(employment_data)


##########################
##### Mobility data ######
##########################

# Source: Google

#Read data
mobility <- read.csv('Data/2020_US_Region_Mobility_Report.csv', header = T, na.strings ="") %>% 
  mutate(date = as.Date(date)) %>% filter(is.na(sub_region_2) & !is.na(sub_region_1)) %>% 
  select(c(date, sub_region_1, retail_and_recreation_percent_change_from_baseline,
           grocery_and_pharmacy_percent_change_from_baseline, parks_percent_change_from_baseline,
           transit_stations_percent_change_from_baseline, workplaces_percent_change_from_baseline,
           residential_percent_change_from_baseline)) %>% 
  'colnames<-'(c('date','state_name','Mob_ret_rec','Mob_gro_pha', 'Mob_parks','Mob_transit','Mob_work','Mob_res'))


#Seasonal adjusted daily data (only taking residential mobility)

mobility <- mobility %>% select(date, state_name, Mob_res) %>% mutate(Mob_res_SA = NA)

## Example for Alabama (weekly seasonal adjusted)
example <- mobility %>% filter(state_name == 'Alabama') %>% select(date, Mob_res)
ts <- ts(example$Mob_res, frequency = 7, start = c(2020,as.numeric(format(as.Date('2020-02-15'), "%j"))))
plot(ts)
decompose_ts <- decompose(ts, 'additive')
plot(decompose_ts)
adjust_ts <- ts - decompose_ts$seasonal
plot(adjust_ts)

## Do for all data
states <- unique(mobility$state_name)
for(i in 1:length(states)){
  state <- states[i]
  
  state_data <- mobility %>% filter(state_name == state) %>% select(date, Mob_res)
  ts <- ts(state_data$Mob_res, frequency = 7, start = c(2020,as.numeric(format(as.Date('2020-02-15'), "%j"))))
  decompose_ts <- decompose(ts, 'additive')
  adjust_ts <- ts - decompose_ts$seasonal
  
  mobility[mobility$state_name == state,]$Mob_res_SA <- adjust_ts
}

#Computing 30 days change (to obtain the monthly change)
mobility <- mobility %>% mutate(delta_Mob_res_SA = NA)
states <- unique(mobility$state_name)
for(i in 1:length(states)){
  state <- states[i]
  
  mob <- mobility %>% filter(state_name == state) %>% select(Mob_res_SA)
  delta_mob <- unname(unlist(mob/lag(mob, n=30)-1))
  
  mobility[mobility$state_name == state,]$delta_Mob_res_SA <- delta_mob
}

unique(employment_data$state_name)
unique(mobility$state_name)
unique(stringency_us$state_name)

head(mobility)
tail(mobility)


#########################
#### Stringency data ####
#########################

#load data
stringency_us <- read.csv("https://raw.githubusercontent.com/OxCGRT/USA-covid-policy/master/data/OxCGRT_US_latest.csv")
stringency_us <- stringency_us %>% select(Date, RegionName, StringencyIndex) %>% filter(RegionName != '')

#Modify name of one state
stringency_us[stringency_us$RegionName == 'Washington DC',]$RegionName <- rep('District of Columbia', length(stringency_us[stringency_us$RegionName == 'Washington DC',]$RegionName))

#Convert dates
stringency_us <- stringency_us %>% mutate(date = as.Date(as.character(Date), '%Y%m%d')) %>% 
  select(date, RegionName, StringencyIndex) %>% 
  'colnames<-'(c('date','state_name', 'stringency_index'))

#Compute monthly change of stringency index
stringency_us <- stringency_us %>% mutate(delta_stringency_index = NA)
states <- unique(stringency_us$state_name)
for(i in 1:length(states)){
  state <- states[i]
  
  str <- stringency_us %>% filter(state_name == state) %>% select(stringency_index)
  delta_str <- unname(unlist(str - lag(str, n=30)))
  
  stringency_us[stringency_us$state_name == state,]$delta_stringency_index <- delta_str
}


head(stringency_us)
tail(stringency_us)


#########################
##### Merge all data ####
#########################

#Merge employment and mobility

data <- merge(employment_data, mobility, by.x=c('date','state_name'))
data <- merge(data, stringency_us, by.x = c('date','state_name'))

head(data)
tail(data)

modified_data <- data
modified_data$emp_monthly_change <- -modified_data$emp_monthly_change
colnames(modified_data)[4] <- c("change_in_unemp")
colnames(modified_data)[5] <- c("Sector")
```

### Figure 3.
```{r echo=FALSE, message = FALSE, warning = FALSE, out.width=c('50%', '50%'), fig.show='hold'}
ggplot(data = modified_data,
       aes(x = date, y = change_in_unemp, color = Sector)) + geom_smooth(se =
                                                                                        FALSE) + theme(legend.position = "none") +labs(x = "Date", y = "Change in Unemployment", title = "Change in Unemployment (All 13 Sectors)")

modified_data2 <- modified_data[modified_data$Sector == "Leisure and Hospitality"|modified_data$Sector == "Financial Activities"|modified_data$Sector == "Retail Trade"|modified_data$Sector == "Government",]

ggplot(data = modified_data2,
       aes(x = date, y = change_in_unemp, color = Sector)) + geom_smooth(se =
                                                                                        FALSE) + theme(legend.position = "bottom") + theme_bw() +labs(x = "Date", y = "Change in Unemployment", title = "Change in Unemployment (Selected Sectors)")
```

```{r echo=FALSE, message = FALSE, warning = FALSE, out.width=c('50%', '50%'), fig.show='hold'}
ggplot(data = modified_data,
       aes(x = date, y = change_in_unemp, color = state_name)) + geom_smooth(se =
                                                                                        FALSE) + theme(legend.position = "none") +labs(x = "Date", y = "Change in Unemployment", title = "Change in Unemployment (All 50 States)")

modified_data3 <- modified_data[modified_data$state_name == "Nevada"|modified_data$state_name == "Arkansas"|modified_data$state_name == "Utah"|modified_data$state_name == "New York",]

ggplot(data = modified_data3,
       aes(x = date, y = change_in_unemp, color = state_name)) + geom_smooth(se =
                                                                                        FALSE) + theme(legend.position = "bottom") + theme_bw() +labs(x = "Date", y = "Change in Unemployment", title = "Change in Unemployment (Selected States)")
```

## 3. Methods
Based on our data analysis, we believe that the change in unemployment for a given state, time, and sector can be predicted using mobility, stringency, and sector composition in the following formulation:

$$\Delta Unemp_{s,t,sec} = \alpha + \mu_{sec} + \beta_{1} Mob_{s,t}^2 + \beta_{2} String_{s,t} + \beta_{3} Mob_{s,t}^2*Sector_{s,t} + \beta_{4} String_{s,t}*Sector_{s,t} + X_{s,t}*\Gamma + \epsilon_{s,t,sec}$$

where

* $\Delta Unemp_{s,t,sec}$ is the change in unemployment for a given state (in the U.S.), at a given time (limited to the pandemic era; 02/01/2020-06/01/2021), for a given sector (one of 13)

* $\alpha$ is the global intercept and $\mu_{sec}$ is the local intercept that varies for each sector.

* $Mob_{s,t}$ is the seasonally adjusted residential mobility of a given state at a given time (so higher when people move less). We included a second degree polynomial term of mobility assuming that greater magnitude of mobility would have more impact on unemployment. (the R-squared without the second degree term was 4pp lower - see appendix)

* $String_{s,t}$ is the stringency level of a given state at a given time (higher index means more stringent)

* $Sector_{s,t}$ is the categorical variable.

* $X_{s,t}$ encapsulates vectors of control variables such as the number of new COVID-19 cases in a given state at a given time. Note that, in this report, control variables are not yet included.

Using data from 2/29/2020 to 1/31/2021, we will derive coefficients in our model using k-fold cross validation (k=10; the choice of k here is arbitrary). Then, using these coefficients, we plan to predict the 2021 April/May change in unemployment for a given state for a given sector. Finally, we will compute the RMSE between the predicted values and the actual values to assess the performance of our model. 

In this project, we do not anticipate our model to necessarily produce the *lowest* RMSE. To minimize RMSE and let our model have the most predictive power, we should include as many features as possible that have any relevance to unemployment. However, taking this approach, the coefficient for the variables of our interest would lose most of their interpretability. Since we are interested in the interpretation of these coefficients as well as the predictive power of our model, we will instead take the approach of including only a handful of control variables in our model.

## 4. Results
In this report, instead of running the cross validation, we ran a linear model using the entire data set (using the formulation above) to first assess the significance of the coefficient of each variable of our interest. 

### Figure 4.
```{r echo=FALSE}
#modify
modified_data_ols <- modified_data 
modified_data_ols$Sector <- factor(modified_data_ols$Sector)
modified_data_ols$Mob_res_SA_sq <- (modified_data_ols$Mob_res_SA)^2

#ols table
ols1 <- lm(change_in_unemp ~ Mob_res_SA_sq*Sector + stringency_index*Sector , data = modified_data_ols)
ols1 <- ols1 %>% tidy()
ols1[,c(2,4,5)] <- sapply(ols1[,c(2,4,5)], round, 4)
ols1 %>% kable()

```

Based on the OLS Table, we noticed a couple of things (note that this is a preliminary analysis of our model; we expect the coefficients will be adjusted once we add more control variables in our model going forward):

* Based on the R-squared, our model could explain 23.9% of variation in the change in unemployment.

* The coefficient for the squared Mobility was statistically significant with a t value of 9.725.

* The coefficient for the stringency was statistically significant with a t value of -2.873.

* The local intercept for the Leisure and hospitality sector, which depends heavily on person-to-person contact, showed statistical significance with a t value of -5.545.

* Most of the coefficients for the interaction term, $Mob_{s,t}^2*Sector_{s,t}$ were statistically significant, while only a handful of the coefficients for the interaction term, $String_{s,t}*Sector_{s,t}$ were statistically significant. This result agrees with a separate linear regression we run in the appendix, which excludes mobility. Based on the low R-squared of 5.6% from that regression, we see that stringency itself is not enough to explain the variation of the unemployment data.


## 5. Conclusion
The project started from the idea that mobility, stringency, and sector composition (and some additional control variables) would sufficiently predict the change in unemployment rate of a given state, for a given sector, at a certain month-end date during the pandemic era.

In the **4.Results** section, we presented a summary of the OLS fit of *change in unemployment*($\Delta Unemp_{s,t,sec}$) on *squared mobility*, *stringency*, and *sector composition*. From the table, we identified the statistical significance of the coefficients for *squared mobility* and *stringency*, verifying that our hypothesis that mobility and stringency have a strong predictive power when it comes to predicting unemployment during the pandemic era.

Going forward, we plan to enhance our model by following the below steps.

1. Explore BLS data directory further to find the number of people in the labor force for a given state, time, and sector. Again, our current model assumes that the labor force is held constant over 2 month. Since 2 month is reasonably short, we do not consider our assumption to be preposterous. However, there is a chance that there are systematically more/less people completely giving up searching for jobs during the pandemic. Since this would induce bias in the coefficients in our model, finding labor force numbers and computing for unemployment rate could be necessary.

2. Add more control variables to our model. Some candidates include: new COVID cases, share of elderly people, 2016 Trump Vote share, and Bartik-style (method of isolating local labor demand changes) predicted job loss.

As discussed, after we enhance our model, we plan to:

1. Run a cross validation to contrive a RMSE-minimizing set of coefficients. 

2. Using these coefficients, predict April/May 2021 change in unemployment data once the data for independent variables become available.

3. Assess our prediction by computing RMSE between the predicted and the real values.

Finally, it is worth noting that we will not be able to make a causal conclusion based on the coefficient we will obtain at the end of the day. This is because we face the endogeneity problem. Namely, the direction of the causal relationship could go both ways when it comes to the relationship between mobility and unemployment. To make a causal claim, we would need to limit our time frame and use addtional econometric tools so that we can treat mobility as a random assignment.

## 6. Appendices
### 6-1. OLS Table without the mobility terms
```{r echo=FALSE}
ols2 <- lm(change_in_unemp ~ stringency_index*Sector , data = modified_data_ols)

ols2 <- ols2 %>% tidy()
ols2[,c(2,4,5)] <- sapply(ols2[,c(2,4,5)], round, 4)
ols2 %>% kable()
```

### 6-2. OLS Table without taking mobility squared
```{r echo=FALSE}
ols3 <- lm(change_in_unemp ~ Mob_res_SA*Sector + stringency_index*Sector , data = modified_data_ols)

ols3 <- ols3 %>% tidy()
ols3[,c(2,4,5)] <- sapply(ols3[,c(2,4,5)], round, 4)
ols3 %>% kable()
```
## 7. Citations 
* https://www.bls.gov/news.release/pdf/empsit.pdf
* https://bcf.princeton.edu/events/bengt-holmstrom-the-seasonality-of-covid-19/
* https://irle.berkeley.edu/files/2020/07/Unemployment-Effects-of-Stay-at-Home-Orders.pdf